import numpy as np
from keras.models import Sequential
from keras.layers import Dense
from keras.optimizers import Adam
from backtesting import Backtest, Strategy

# Genetic Algorithm parameters
population_size = 50
mutation_rate = 0.01
num_generations = 50

# Neural Network parameters
input_shape = (14, 25)
output_size = 3
learning_rate = 0.001

# Function to create an initial population
def create_population():
    population = []
    for _ in range(population_size):
        strategy = np.random.uniform(-1, 1, size=(np.prod(input_shape), output_size))
        population.append(strategy)
    return population

# Function to evaluate the fitness of each individual in the population
def evaluate_population(population):
    fitness_scores = []
    for strategy in population:
        returns = backtest_strategy(strategy)
        fitness_scores.append(returns)
    return fitness_scores

# Function to perform crossover between two parent strategies
def crossover(parent1, parent2):
    child = np.zeros_like(parent1)
    for i in range(parent1.shape[0]):
        for j in range(parent1.shape[1]):
            if np.random.rand() < 0.5:
                child[i, j] = parent1[i, j]
            else:
                child[i, j] = parent2[i, j]
    return child

# Function to perform mutation on an individual strategy
def mutate(strategy):
    for i in range(strategy.shape[0]):
        for j in range(strategy.shape[1]):
            if np.random.rand() < mutation_rate:
                strategy[i, j] += np.random.uniform(-0.1, 0.1)
    return strategy

# Function to select the best individuals from the population as parents for the next generation
def select_parents(population, fitness_scores):
    sorted_indices = np.argsort(fitness_scores)
    selected_indices = sorted_indices[-int(population_size / 2):]
    parents = [population[i] for i in selected_indices]
    return parents

# Function to generate the next generation
def generate_next_generation(parents):
    next_generation = []
    while len(next_generation) < population_size:
        parent1 = np.random.choice(parents)
        parent2 = np.random.choice(parents)
        child = crossover(parent1, parent2)
        child = mutate(child)
        next_generation.append(child)
    return next_generation

# Manually normalize the fitness scores
def normalize_fitness(fitness_scores):
    min_score = min(fitness_scores)
    max_score = max(fitness_scores)
    normalized_scores = [(score - min_score) / (max_score - min_score) for score in fitness_scores]
    return normalized_scores

# Function to backtest a strategy and calculate returns
def backtest_strategy(strategy):
    class MyStrategy(Strategy):
        def init(self):
            self.signal = None

        def next(self):
            signal = np.argmax(self.data.close)
            self.signal = signal

    strategy = np.array(strategy).reshape(input_shape + (output_size,))
    data = ...  # Load your stock price data here

    bt = Backtest(data, MyStrategy)
    result = bt.run()
    returns = result["returns"].iloc[-1]

    return returns

# Main genetic algorithm loop
def genetic_algorithm():
    population = create_population()
    for generation in range(num_generations):
        print(f"Generation {generation + 1}/{num_generations}")
        fitness_scores = evaluate_population(population)
        normalized_fitness = normalize_fitness(fitness_scores)

        parents = select_parents(population, normalized_fitness)
        population = generate_next_generation(parents)

    # Evaluate the final population and return the best strategy
    fitness_scores = evaluate_population(population)
    best_strategy_index = np.argmax(fitness_scores)
    best_strategy = population[best_strategy_index]
    return best_strategy

# Run the genetic algorithm
best_strategy = genetic_algorithm()
print("Best strategy:", best_strategy)
